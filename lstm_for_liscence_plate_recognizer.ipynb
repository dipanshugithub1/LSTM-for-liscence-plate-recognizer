{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "042fa08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CTCLoss\n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f62171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/maximus1/Downloads/archive (2)/cropped_lps/cropped_lps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ec28a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualising few images from the directory\n",
    "disp_counter=0\n",
    "for file_name in glob.glob(dataset_path+\"/*.jpg\"):\n",
    "    img = cv2.imread(file_name)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    disp_counter+=1\n",
    "    if disp_counter==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58d82f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset\n",
    "class VehicleDataset(Dataset):\n",
    "    def __init__(self,dataset_path, transform=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.transform = transform\n",
    "        self.images,self.labels = [],[]\n",
    "        for file_name in glob.glob(dataset_path+\"cropped_lps/cropped_lps/*.jpg\"):\n",
    "            self.images.append(Image.open(file_name))\n",
    "            label_id = file_name.split('/')[-1]\n",
    "            labels_mapper = pd.read_csv(dataset_path+\"lpr.csv\")\n",
    "            index = labels_mapper.loc[labels_mapper['images']==label_id].index\n",
    "            self.labels.append(labels_mapper['labels'][index].values[0])\n",
    "     \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        img = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        item = {'img':img, 'label':label}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1d511ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a collate function #optional that could other wise do padding\n",
    "class Collater(object):\n",
    "\n",
    "    def __init__(self,arg=None):\n",
    "        self.arg = arg\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        for b in batch:\n",
    "            img,label = b['img'],b['label']\n",
    "\n",
    "        item = {}\n",
    "        item['img'] = img\n",
    "        item['label'] = label\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65202e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/maximus1/Downloads/archive (2)/\"\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Resize((100,32)), transforms.Grayscale(num_output_channels=1)])\n",
    "train_data = VehicleDataset(dataset_path=dataset_path,transform=transform) #alternatively,arg=transforms =transform instead of collate_fn can be passsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "358da9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,batch in enumerate(train_data):\n",
    "    print(batch['img'], batch['label'])\n",
    "    print(batch['img'].dtype)\n",
    "    plt.imshow(batch['img'][0])\n",
    "    plt.show()\n",
    "    if i==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b10ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating data loader\n",
    "batch_size=8\n",
    "dataloader = DataLoader(train_data, batch_size=8, shuffle=True, num_workers=4, collate_fn=Collater())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50cd3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = string.ascii_uppercase + string.digits\n",
    "print(alphabets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca462f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions for converting string label\n",
    "class strLabelConverter(object):\n",
    "    \"\"\" Class for converting str and label\n",
    "    blank should be inserted to the alphabets for CTC\n",
    "    ignoring case = True, since number plate's char is upper case\n",
    "    \"\"\"\n",
    "    def __init__(self,alphabet, ignore_case=True):\n",
    "        self.ignore_case =ignore_case\n",
    "        \n",
    "        self.alphabet = alphabet + '-' # at last index\n",
    "        self.dict = {}\n",
    "        for i, char in enumerate(alphabet):\n",
    "            self.dict[char]=i+1\n",
    "\n",
    "    #encoding \n",
    "    def encode(self,text):\n",
    "        length = []\n",
    "        result = []\n",
    "        for item in text:\n",
    "            item = item.encode().decode('utf-8','strict')\n",
    "            length.append(len(item))\n",
    "            r = []\n",
    "            for char in item:\n",
    "                index = self.dict[char]\n",
    "                r.append(index)\n",
    "            result.append(r)\n",
    "            #converting each char to their relative numbers\n",
    "        \n",
    "        max_len =0\n",
    "        for r in result:\n",
    "            if len(r)>max_len:\n",
    "                max_len = len(r)\n",
    "        \n",
    "        result_temp = []\n",
    "        for r in result:\n",
    "            for i in range(max_len - len(r)):\n",
    "                r.append(0)\n",
    "            result_temp.append(r)\n",
    "        \n",
    "        text = result_temp\n",
    "        return (torch.LongTensor(text), torch.LongTensor(length))\n",
    "        # since CTC expects long tensor of encoded text and its seq_length\n",
    "        # [a,b,c], len = [0,1,2], [3]\n",
    "    #decoding\n",
    "\n",
    "    def decode(self, t, length, raw=False):\n",
    "        '''\n",
    "        Decode encoded texts back into strs\n",
    "        '''\n",
    "        if length.numel() == 1:\n",
    "            length = length[0]\n",
    "            assert t.numel() == length, \"text with length: {} does not match declared length: {}\".format(t.numel(), length)\n",
    "            if raw:\n",
    "                return ''.join([self.alphabet[i - 1] for i in t])\n",
    "            else:\n",
    "                char_list = []\n",
    "                for i in range(length):\n",
    "                    if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):\n",
    "                        char_list.append(self.alphabet[t[i] - 1])\n",
    "                return ''.join(char_list)\n",
    "        else:\n",
    "            # batch mode\n",
    "            assert t.numel() == length.sum(), \"texts with length: {} does not match declared length: {}\".format(t.numel(), length.sum())\n",
    "            texts = []\n",
    "            index = 0\n",
    "            for i in range(length.numel()):\n",
    "                l = length[i]\n",
    "                texts.append(\n",
    "                    self.decode(\n",
    "                        t[index:index + l], torch.LongTensor([l]), raw=raw))\n",
    "                index += l\n",
    "            return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6065992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to one hot\n",
    "def one_hot(text, text_length, nc):\n",
    "    batch_size = text_length.size(0)\n",
    "    maxLength = text_length.max()\n",
    "    onehot = torch.FloatTensor(batch_size,maxLength, nc).fill_(0)\n",
    "    acc = 0\n",
    "    for i in range(batch_size):\n",
    "        length = text_length[i]\n",
    "        label = text[acc:acc+length].view(-1,1).long()\n",
    "        one_hot[i,:length].scatter_(1,label,1.0)\n",
    "        acc+=length\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd8c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRNN model -https://github.com/meijieru/crnn.pytorch/blob/master/models/crnn.py\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, imgH, nc, nclass, nh, n_rnn=2, leakyRelu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "        assert imgH % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = nc if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        convRelu(0)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n",
    "        convRelu(1)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "        cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
    "        convRelu(4, True)\n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
    "        convRelu(6, True)  # 512x1x16\n",
    "\n",
    "        self.cnn = cnn\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, nh, nh),\n",
    "            BidirectionalLSTM(nh, nh, nclass))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "        conv = self.cnn(input)\n",
    "        b, c, h, w = conv.size()\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "\n",
    "        # rnn features\n",
    "        output = self.rnn(conv)\n",
    "        \n",
    "        # add log_softmax to converge output\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def backward_hook(self, module, grad_input, grad_output):\n",
    "        for g in grad_input:\n",
    "            g[g != g] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f0d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = len(alphabets)+1\n",
    "\n",
    "crnn = CRNN(imgH=32,nc=1, nclass=n_class,nh=256) # nc = num channel nh = size of lstm hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ac48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising weights\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f62401",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df74bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in crnn.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97283164",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_decoder = strLabelConverter(alphabet=alphabets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b31f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(crnn.parameters(), lr=0.01)\n",
    "criterion = CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c0c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainiing\n",
    "crnn.train()\n",
    "num_epochs=10\n",
    "for epoch in range(num_epochs):\n",
    "    for i,batch in enumerate(dataloader):\n",
    "        img,label = batch['img'],batch['label']\n",
    "        img = img.unsqueeze(0)\n",
    "        print(img.shape)\n",
    "        batch_size = img.size(0)\n",
    "        text,len = encoder_decoder.encode(label)\n",
    "        text = text.to(device)\n",
    "        len = len.to(device)\n",
    "        pred = crnn(img)\n",
    "        optimizer.zero_grad()\n",
    "        preds_size = torch.LongTensor([pred.size(0)]* batch_size)\n",
    "        loss = criterion(pred,text, preds_size,len)/batch_size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"Epoch - {}, Loss - {}\".format(epoch, loss()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
